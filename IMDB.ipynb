{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# IMDB Dataset of 50K Movie Reviews\n",
        "\n"
      ],
      "metadata": {
        "id": "nqwgDlDVEsCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this notebook, we perform a sentiment analysis on a Kaggle dataset of movie reviews. Here is the description from Kaggle:\n",
        "\n",
        "> *IMDB dataset having 50K movie reviews for natural language processing or Text analytics. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training and 25,000 for testing. So, predict the number of positive and negative reviews using either classification or deep learning algorithms.*\n",
        "\n",
        "\n",
        "Our approach features a recurrent neural network with a long short-term memory (LSTM) cell. Step-by-step:\n",
        "\n",
        "* Obtain the data from Kaggle.\n",
        "\n",
        "* Split the data into train and test set as indicated (25K each).\n",
        "\n",
        "* Preprocess the data by tokenizing words and forming a vocabulary.\n",
        "\n",
        "* Form the LSTM neural network model.  \n",
        "\n",
        "* Train the model using stochastic gradient descent. Due to GPU constraints, I was not able to train on the entire data set.\n",
        "\n",
        "* Evaluate the model on the test set.\n",
        "\n",
        "We begin with standard imports:"
      ],
      "metadata": {
        "id": "fb96xmD5hU5D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PMTsjMFVehwY"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from matplotlib import pyplot as plt\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Obtain the data from Kaggle\n",
        "\n",
        "The first step is to obtain the data from Kaggle. This notebook is intended to be run in Google CoLab, so the first step is to mount google drive:\n",
        "\n",
        "<!-- We follow the instructions on this [page](https://towardsdatascience.com/downloading-kaggle-datasets-directly-into-google-colab-c8f0f407d73a).  -->"
      ],
      "metadata": {
        "id": "K5NvKwkZIKOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSkHqQZV_S2Y",
        "outputId": "3d79e604-afc0-4942-f93b-ff1bb8254505"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We set the Kaggle configuration directory to be where the kaggle.json token is located."
      ],
      "metadata": {
        "id": "UbKVH6cTHdGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/gdrive/MyDrive/kaggle'"
      ],
      "metadata": {
        "id": "KPv9enSs_cqv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the movie reviews data (this requires installation of the kaggle package via `pip install kaggle`, if necessary).\n"
      ],
      "metadata": {
        "id": "6JgQP72HHlJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "up6FxQUWAZAz",
        "outputId": "f665ed24-d148-40ea-d788-3f2a433cbd25"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imdb-dataset-of-50k-movie-reviews.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, copy the zip to the virtual machine and unzip it there.\n"
      ],
      "metadata": {
        "id": "CaE6m6OtHvti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = '/gdrive/MyDrive/kaggle/imdb-dataset-of-50k-movie-reviews.zip'\n",
        "!cp '{zip_path}' .\n",
        "!unzip -q 'imdb-dataset-of-50k-movie-reviews.zip'"
      ],
      "metadata": {
        "id": "3ESmJDY_BpO8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed8fcc22-f751-4440-b179-e77e158dc8fc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace IMDB Dataset.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the working directory to see that the necessary files are there.\n"
      ],
      "metadata": {
        "id": "ATlbs5xgH5Ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqN_M0ZXF8IE",
        "outputId": "0983ab2f-4d40-4488-f036-63428aede208"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'IMDB Dataset.csv',\n",
              " 'imdb-dataset-of-50k-movie-reviews.zip',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Preprocessing\n",
        "\n",
        "To process the data, the first step is to read the data into a pandas dataframe."
      ],
      "metadata": {
        "id": "9d44PVprH9va"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QqJ2vkbqehwa"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"IMDB Dataset.csv\", lineterminator='\\n', converters={\"review\": str(), \"sentiment\": str()})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we split the data into train and test sets. Following the description on Kaggle, the split is 50/50.\n"
      ],
      "metadata": {
        "id": "c0kbsM8gxgGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(num_samples*0.5)\n",
        "test_size = len(data) - train_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "O9N5qYzTxe_I",
        "outputId": "aed66351-4529-4022-8998-952118b69481"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-11325776c6d5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_samples' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shuffle all the data (`fraction = 1`), and split the shuffled data into the train and test sets."
      ],
      "metadata": {
        "id": "VTpkmFhkx5Z_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_data = data.sample(frac=1)\n",
        "train_data, test_data = shuffled_data[:25000].copy(), shuffled_data[25000:].copy()\n",
        "train_data"
      ],
      "metadata": {
        "id": "8nBMM4LCx58l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `VocabFromReviews` encapsulates the main prepocessing steps. The words in the training data reviews constitute the tokens in our vocabulary. We order them by frequency and form a dictionary between indices and tokens. The `VocabFromReviews` class includes the method `process_and_convert_review_to_tensor` which uses the vocabulary to tokenize, index, and pad any pandas Series of reviews; it outputs a pytorch tensor."
      ],
      "metadata": {
        "id": "KXq6OghrI2ru"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1AvXXx7ehwb"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import itertools\n",
        "import re\n",
        "\n",
        "\n",
        "class VocabFromReviews:\n",
        "    \"\"\"\n",
        "    The Vocab takes a pd.Series of reviews, processes them into tokens by descending frequency, and creates dictionaries to move between tokens and indices\n",
        "    \"\"\"\n",
        "    def __init__(self, reviews: pd.Series, min_freq: int = 0):\n",
        "      tokenized_series = reviews.apply(\n",
        "          lambda review_text : [self.preprocess_string(word) for word in review_text.split()]\n",
        "      )\n",
        "      tokenized_list = tokenized_series.to_list()\n",
        "      tokens = list(itertools.chain.from_iterable(tokenized_list))\n",
        "      counts = Counter(tokens)\n",
        "      self.token_freqs = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
        "      self.idx_to_token = list(sorted(set(\n",
        "          ['<unk>'] + [token for token, freq in self.token_freqs if freq >= min_freq])))\n",
        "      self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
        "\n",
        "    def preprocess_string(self, s: str):\n",
        "      \"\"\" Keep only words, and make them lower case. Remove breaks.\"\"\"\n",
        "      s = re.sub(r\"[^\\w\\s]\", '', s).lower()\n",
        "      s = re.sub(r\"\\s+\", '', s)\n",
        "      s = re.sub(r\"\\d\", '', s)\n",
        "      if s == \"br\": return \"\"\n",
        "      return s\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.idx_to_token)\n",
        "\n",
        "    def convert_tokenized_review_to_indices(self, single_tokenized_review : list):\n",
        "      indices = []\n",
        "      for token in single_tokenized_review:\n",
        "          if token in self.token_to_idx and token:\n",
        "              indices.append(self.token_to_idx[token])\n",
        "      return indices\n",
        "\n",
        "\n",
        "    def process_and_convert_review_to_tensor(self, input_reviews: pd.Series):\n",
        "      \"\"\"\n",
        "      Take a pd.Series of reviews, tokenize, index according to the vocab dictionary, pad, and convert to a tensor\n",
        "      \"\"\"\n",
        "      indexed_series = input_reviews.apply(\n",
        "          lambda review_text :\n",
        "            self.convert_tokenized_review_to_indices(\n",
        "              [self.preprocess_string(word) for word in review_text.split()]\n",
        "          )\n",
        "      )\n",
        "      max_length = indexed_series.apply(lambda l : len(l)).max()\n",
        "      padded_indexed_series = indexed_series.apply(\n",
        "        lambda review_indices : [0]*(max_length- len(review_indices)) + review_indices\n",
        "      )\n",
        "      return torch.tensor(padded_indexed_series.values.tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now create the vocab from the training data."
      ],
      "metadata": {
        "id": "1Hf24ASQJOYH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_fph1XBehwc"
      },
      "outputs": [],
      "source": [
        "vocab = VocabFromReviews(train_data[\"review\"], min_freq=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process the training data into padded indexed tokens."
      ],
      "metadata": {
        "id": "0tqGiwIrzQck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features = vocab.process_and_convert_review_to_tensor(train_data[\"review\"])\n",
        "print(f\"size of train features = {train_features.size()}\")"
      ],
      "metadata": {
        "id": "aMOxAEIT0sa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the labels, encode a positive sentiment as 1 and a negative one as 0."
      ],
      "metadata": {
        "id": "XwZ_1uqRzZRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels_pd = train_data[\"sentiment\"].apply(lambda s: int(s == \"positive\"))\n",
        "train_labels = torch.tensor(train_labels_pd.values.tolist()).unsqueeze(1).float()\n",
        "print(f\"size of train labels = {train_labels.size()}\")"
      ],
      "metadata": {
        "id": "cEQs34wD4p2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI7zQbHhehwd"
      },
      "source": [
        "\n",
        "## 3. Model\n",
        "\n",
        "We now define the recurrent neural network model. The model is many-to-one since we input a tokenized string but output only a single value (positive/negative). Hence, we use a hidden recurrent neural network that outputs a single value at the end, and this value is plugged into a fully connected neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjJe8O1vehwd"
      },
      "outputs": [],
      "source": [
        "class RNNHidden(nn.Module):\n",
        "    def __init__(self, num_layers, input_dim, embedding_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first = True)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #text.size() = (batch size, length of sequence)\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        #embedded.size() = (batch_size, length of sequence)\n",
        "        _, hidden = self.lstm(embedded)\n",
        "\n",
        "        # Since this is a Many-to-One model, return only the last output\n",
        "        return hidden[-1][0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "OUTPUT_DIM = 1\n",
        "HIDDEN_LAYERS = 2\n",
        "\n",
        "lstm_model = nn.Sequential(\n",
        "    RNNHidden(HIDDEN_LAYERS, INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM),\n",
        "    nn.Linear(HIDDEN_DIM, OUTPUT_DIM),\n",
        "    nn.Dropout(p=0.5)\n",
        ")\n",
        "print(lstm_model)"
      ],
      "metadata": {
        "id": "W5lCoTwtz3hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OZy2jmTehwe"
      },
      "source": [
        "## 4. Training\n",
        "\n",
        "Before training, we check is there is a GPU available. If so, the device will be the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zF3cBjikehwe"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, define a binary accuracy function for evaluation."
      ],
      "metadata": {
        "id": "iahXPzFE0YW2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ie_4-Bp2ehwf"
      },
      "outputs": [],
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We arrive at the training loop for the network."
      ],
      "metadata": {
        "id": "wblYR86rsMQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, num_epochs, verbose=False):\n",
        "\n",
        "    # Keep track of the loss and accuracy over the epochs\n",
        "    train_loss = []\n",
        "    train_acc = []\n",
        "\n",
        "    model.train()\n",
        "    for i in tqdm.tqdm(range(num_epochs)):\n",
        "      for features, labels  in dataloader:\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(features)\n",
        "        loss = criterion(predictions, labels)\n",
        "        acc = binary_accuracy(predictions, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss.append(loss.item())\n",
        "        train_acc.append(acc.item())\n",
        "\n",
        "      if verbose:\n",
        "        print(f\"Epoch {i+1}: loss = {round(train_loss[-1],4)}, accuracy = {round(train_acc[-1],4)}\")\n",
        "\n",
        "    return train_loss, train_acc"
      ],
      "metadata": {
        "id": "TB1U59Muw2Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a Dataset class for the review data. This is so that we can use the dataloader to create batches automatically."
      ],
      "metadata": {
        "id": "nldnVZFX02CB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MovieReviewsDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "riVI-_X-0uSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the training dataset and the training dataloader."
      ],
      "metadata": {
        "id": "czeSc2U11AJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MovieReviewsDataset(train_features.to(device), train_labels.to(device))\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "id": "jSCHtFDB002B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a stochastic gradient descent optimzer. The loss function is the binary cross entropy combined with sigmoid."
      ],
      "metadata": {
        "id": "dXOqN8_Qs56A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVNmHGG2ehwf"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "lstm_model = lstm_model.to(device)\n",
        "optimizer = optim.SGD(lstm_model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We finally get to training the model!"
      ],
      "metadata": {
        "id": "yfvEgsJPtHwS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcjgJcwsehwg"
      },
      "outputs": [],
      "source": [
        "loss, acc = train(lstm_model, train_loader,\n",
        "    optimizer=optimizer, criterion=criterion, num_epochs=50, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now plot how the accuracy and loss change over the course of training."
      ],
      "metadata": {
        "id": "MKN9-iu_tKhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(acc)\n",
        "plt.xlabel(\"batch\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.title(\"Plot of accuracy during training\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x1RW8HhLKkD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"Plot of loss during training\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CdEoSvbc9oCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "\n",
        "Finally, evaluate the model on the test set. We first have the evaluation function:"
      ],
      "metadata": {
        "id": "LF9i2k4-tPqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, features, labels, criterion, verbose=False):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(features)\n",
        "        loss = criterion(predictions, labels)\n",
        "        acc = binary_accuracy(predictions, labels)\n",
        "    if verbose:\n",
        "      print(f\"The test loss is {round(loss.item(), 4)}\")\n",
        "      print(f\"The test accuracy is {round(acc.item(), 4)}\")\n",
        "    return loss , acc"
      ],
      "metadata": {
        "id": "uT8yNhmsnM7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we obtain the test features and labels."
      ],
      "metadata": {
        "id": "SoV5QOQw2lXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_features = vocab.process_and_convert_review_to_tensor(test_data[\"review\"])\n",
        "print(f\"size of test features = {test_features.size()}\")\n",
        "\n",
        "test_labels_pd = test_data[\"sentiment\"].apply(lambda s: int(s == \"positive\"))\n",
        "test_labels = torch.tensor(test_labels_pd.values.tolist()).unsqueeze(1).float()\n",
        "print(f\"size of test labels = {test_labels.size()}\")"
      ],
      "metadata": {
        "id": "otuh8nk_v6Y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, evaluate the model on the test set. We need to do this with only part of the data"
      ],
      "metadata": {
        "id": "IoK5GiYV27rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_list = []\n",
        "test_acc_list = []\n",
        "i = 0\n",
        "while i+500 < len(test_features):\n",
        "  test_loss, test_acc = evaluate(\n",
        "    model= lstm_model,\n",
        "    features= test_features[i:i+500].to(device),\n",
        "    labels = test_labels[i:i+500].to(device),\n",
        "    criterion=criterion,\n",
        "    verbose = False\n",
        "  )\n",
        "  test_loss_list.append(test_loss.item())\n",
        "  test_acc_list.append(test_acc.item())\n",
        "  i += 500\n",
        "\n",
        "mean_loss = sum(test_loss_list)/ len(test_loss_list)\n",
        "mean_acc = sum(test_acc_list)/ len(test_acc_list)\n",
        "\n",
        "print(f\"The test loss is {round(mean_loss, 4)}\")\n",
        "print(f\"The test accuracy is {round(mean_acc, 4)}\")\n"
      ],
      "metadata": {
        "id": "hASdF855EmDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xWnLl8Xt13gj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}